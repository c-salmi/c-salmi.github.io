<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>Gaussian Discriminative Analysis | ME</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css">
<script defer src="/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>













<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto&#43;Serif&#43;SC:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js"
     crossorigin></script>
<link rel="stylesheet" href="/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="/js/fontawesome.min.f14def4f6943de5b19b19c02e239b98c8cc6a8fe66b6828a21da7c0e9d405df8364cab68aa637c3bbe735eb7c325aa3c.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
   integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" 
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
   integrity="sha384-&#43;XBljXPPiv&#43;OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>


<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="If I sample many many apples from a basket of independent identically distributed (i.i.d.) randomly sized appels, the distribution of the size of my apples will converge towards the Gaussian distribution. This is what Laplace came up with in 1812, in his “Analytic Theory of Probability”. Although Carl Friedrich Gauss 3 years before that already introduced the function and worked out the least squares approximation to estimate the &rsquo;true&rsquo; (mean) value from a bunch of i.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Gaussian Discriminative Analysis",
      "item":"/posts/2-gaussian-discriminative-analysis/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/2-gaussian-discriminative-analysis/"
    },
    "headline": "Gaussian Discriminative Analysis | ME","datePublished": "2019-12-20T20:24:51+01:00",
    "dateModified": "2019-12-20T20:24:51+01:00",
    "wordCount":  909 ,
    "publisher": {
        "@type": "Person",
        "name": "WANG Chucheng",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "If I sample many many apples from a basket of independent identically distributed (i.i.d.) randomly sized appels, the distribution of the size of my apples will converge towards the Gaussian distribution. This is what Laplace came up with in 1812, in his “Analytic Theory of Probability”. Although Carl Friedrich Gauss 3 years before that already introduced the function and worked out the least squares approximation to estimate the \u0026rsquo;true\u0026rsquo; (mean) value from a bunch of i."
}
</script><meta property="og:title" content="Gaussian Discriminative Analysis | ME" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/2-gaussian-discriminative-analysis/" />




<meta property="og:description" content="If I sample many many apples from a basket of independent identically distributed (i.i.d.) randomly sized appels, the distribution of the size of my apples will converge towards the Gaussian distribution. This is what Laplace came up with in 1812, in his “Analytic Theory of Probability”. Although Carl Friedrich Gauss 3 years before that already introduced the function and worked out the least squares approximation to estimate the &rsquo;true&rsquo; (mean) value from a bunch of i." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="ME" />






<meta property="article:published_time" content="2019-12-20T20:24:51&#43;01:00" />


<meta property="article:modified_time" content="2019-12-20T20:24:51&#43;01:00" />



<meta property="article:section" content="posts" />





  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">ME</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">About</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">Posts</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">Light</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8">
  
  
  <div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12">
    <div
      class="lg:col-start-2 bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"
    >
      <article class="prose">
  <h1 class="mb-4">Gaussian Discriminative Analysis</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2019-12-20</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>5 min read</span>
  </div>

  

  
</div>


  
  

  <p>If I sample many many apples from a basket of independent identically distributed (i.i.d.) randomly sized appels, the distribution of the size of my apples will converge towards the Gaussian distribution. This is what Laplace came up with in 1812, in his “Analytic Theory of Probability”. Although Carl Friedrich Gauss 3 years before that already introduced the function and worked out the least squares approximation to estimate the &rsquo;true&rsquo; (mean) value from a bunch of i.i.d. measurements (very usefull in statistics / confidence calculations).</p>
<p><strong>1D Gaussian</strong></p>
<div>
\[\begin{aligned}
      \mathcal{N}(\mu, \sigma^{2}) = \dfrac{1}{\sigma \sqrt{2\pi}} \exp{(-\dfrac{1}{2} ( \dfrac{x-\mu}{\sigma})^{2})}
\end{aligned}\]
</div>
<p><img src="/images/Gauss1D.png" alt="img"></p>
<p>Now lets get back to machine learning &hellip;. We can use a the Gaussian distribution to model the PDF of features. And if you remember, we can use Bayes&rsquo; theorom to caculate the posterior distribution from the PDF and so we can make a prediction algorithm!</p>
<p><strong><em>Parametric Density Estimation</em></strong>: To get an idea of the underlying distribution of the features we try to estimate this distribution based on a formula with parameters. Most commenly used formula for parametric density estimation is the Gaussian distribution:</p>
<h3 id="quadratic-discriminate-analysis">Quadratic discriminate analysis</h3>
<p>Lets say we only have two features of Fishers&rsquo; Iris dataset available, for example petal length and sepal length and we are only dealing with two species instead of three (making the example as easy as possible to visualize it).</p>
<p><img src="/images/small_iris.png" alt="img"></p>
<p>The Quadratic discriminate analysis is an algorithm that predicts the species based on the two features. This does require the multivariate version of the Gaussian so let&rsquo;s have a look:</p>
<p><strong>Multivariate Gaussian</strong></p>
<div>
\[\begin{aligned}
      \mathcal{N}(\mu, \Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma^{1/2}| } \exp{(-\dfrac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x-\mu) )}
\end{aligned}\]
</div>
<p><img src="/images/Gauss2D.png" alt="img"></p>
<p>In binary (two species) QDA the classification is based on the discriminant:</p>
<div>
\[\begin{aligned}
    f(x) = \text{log} \ p(y_1 | x) - \text{ log } \ p(y_2 | x)
\end{aligned}\]
</div>
<p>A positive discriminant means the posterior probability of class 1 is higher and thus we classify $x$ as class 1. The log of the posterior probabilities is taken because this yields a simpler (more efficient) equation. This is possible because log is a monotonically increasing function, which means that the relations of order are unchanged. The $max_i$ $p(y_i|x)$ will also be the $max_i$ log $p(y_i|x)$. From the log of the posterior probability follows:</p>
<div>
\[\begin{aligned}
    p(y_i|x) &= \dfrac{p(x|y_i)p(y_i)}{p(x)} \\
    \log{p(y_i|x)} &= \log{p(x|y_i)} + \log{p(y_i)} - \log{p(x)} \\
    &= -\dfrac{p}{2}\log{2\pi} -\dfrac{1}{2}\log{\det{\Sigma_i}} - \dfrac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) \\
    &+ \log{p(y_i)} - \log{p(x)} \\
\end{aligned}\]
</div>
<p>Removing all non-class dependent constants yields:</p>
<div>
\[\begin{aligned}
    \log{p(y_i|x)} &= -\dfrac{1}{2}\log{\det{\Sigma_i}} - \dfrac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) + \log{p(y_i)}
\end{aligned}\]
</div>
<p>Plugging the result into the discriminant equation yields:</p>
<div>
\[\begin{aligned}
    f(x) &= \log{p(y_1|x)} - \log{p(y_2|x)} \\
    f(x) &= x^T W x + w^T x + w_0 \\
    W &= \dfrac{1}{2} (\Sigma_2^{-1} - \Sigma_1^{-1}) \\
    w &= \mu_1^T\Sigma_1^{-1} - \mu_2^T\Sigma_2^{-1} \\
    w_0 &= -\dfrac{1}{2}\log{\det{\Sigma_1}} - \dfrac{1}{2}\mu_1^T\Sigma_1^{-1}\mu_1 + \log{p(y_1)} \\
    &+ \dfrac{1}{2}\log{\det{\Sigma_2}} + \dfrac{1}{2}\mu_2^T\Sigma_2^{-1}\mu_2 - \log{p(y_2)} \\
\end{aligned}\]
</div>
<p>This is a quadratic equation modeling the decision boundary for a two class classification problem. The final class prediction or hypothesis $h(x)$ is then given by the following bernouilli distribution.</p>
<h3 id="linear-discriminate-analysis">Linear discriminate analysis</h3>
<p>Given the assumption that the covariance matrices of both classes are equal $(\Sigma_1 == \Sigma_2)$, the decision boundary becomes
linear:</p>
<div>
\[\begin{aligned}
    f(x) &= w^T x + w_0 \\
    w &= \hat{\Sigma}^{-1} (\hat{\mu}_2 - \hat{\mu}_1) \\
    w_0 &= \dfrac{1}{2} \hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 - \dfrac{1}{2} \hat{\mu}_1^T \hat{\Sigma}^{-1} \hat{\mu}_1 + \log{\dfrac{p(y_1)}{p(y_2)}} \\
\end{aligned}\]
</div>
<p>Also called Fisher&rsquo;s Linear Discriminate analysis.</p>
<h4 id="fisher-iris-example">Fisher Iris Example</h4>
<p><img src="/images/qda_vs_lda.png" alt="img"></p>
<p>Code for this was taken from: <a href="https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py">https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py</a></p>
<h4 id="side-note-lda-as-a-dimensionality-reduction-technique">Side Note: LDA as a dimensionality reduction technique</h4>
<p>In Fisher&rsquo;s article he observes that at the decision boundary (discriminant = 0), the ratio of between and within class
variance is:</p>
<div>
\[\begin{aligned}
    \Phi = \dfrac{\text{variance between}}{\text{variance within}} = \dfrac{[\omega \mu_1 - \omega \mu_0]^2}{\omega^T \Sigma_1 \omega + \omega^T \Sigma_0 \omega}
\end{aligned}\]
</div>
<p>Here $\omega$ is a unit vector in a certain direction. He furthermore obeserves that this ratio is maximal for, $\omega$ as in LDA:</p>
<div>
\[\begin{aligned}
    \omega &= \hat{\Sigma}^{-1} (\hat{\mu}_2 - \hat{\mu}_1) \\
\end{aligned}\]
</div>
<p>The simple interpretation of this is that, the dicision boundary has a slope which is equal to the direction of maximum $\Phi$.</p>
<p>The use LDA as a dimensionality reduction technique all you have to do is map the data onto the first $k$ principal axes of the $\Phi$. (Later on we will have a look at other similar dimensionality reduction technique such as PCA)</p>
<h2 id="conclusion">Conclusion</h2>
<p>We&rsquo;ve looked at two variants of parametric generative algorithms to predict flower species from petal / sepal lengths. For our small test example lda and qda were quite alright, but it&rsquo;s good to know what the limitations are.</p>
<h3 id="qda-assumptionslimitations">QDA assumptions/limitations:</h3>
<ul>
<li>It assumes the PDF of the class conditional distributions can be modelled by a Gaussian.</li>
<li>The decision boundary generated by QDA is quadratic.</li>
<li>It uses Bayes&rsquo; rule to get from class conditional distributions to posterior distribution and to achieve this the class priors are used. So for datasets with imbalanced example count per class this can induce a bias for a specific class with a higher example count.</li>
</ul>
<h3 id="lda-assumptionslimitations">LDA assumptions/limitations:</h3>
<ul>
<li>Like Qda, it assumes the PDF of the class conditional distributions can be modelled by a Gaussian, but it goes one step further by assuming that all classes share a PDF with the same covariance matrix.</li>
<li>The decision boundary generated by LDA is linear.</li>
<li>It uses Bayes&rsquo; rule to get from class conditional distributions to posterior distribution and to achieve this the class priors are used. So for datasets with imbalanced example count per class this can induce a bias for a specific class with a higher example count.</li>
</ul>

</article>


      

      



      

      
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
        <span class="text-primary-text block font-bold"
          >Previous</span
        >
        <a href="/posts/3-non-parmetric-algorithms/" class="block">Non Parametric Algorithms</a>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
        <span class="text-primary-text block font-bold">Next</span>
        <a href="/posts/1-machine-learning/" class="block">Machine Learning</a>
      
    </div>
  </div>


      



    </div>
    

    
    
  </div>

  
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        hljs.highlightAll();
      });
    </script>

          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://www.wangchucheng.com/">WANG Chucheng</a> and <a href="https://www.ruiqima.com/">MA Ruiqi</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
    </footer>
  </body>
</html>

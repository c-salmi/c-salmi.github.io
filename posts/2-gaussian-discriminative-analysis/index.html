<!DOCTYPE html>
<html lang='en' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>Gaussian Discriminative Analysis | ME</title>
<link rel="stylesheet" href="/css/eureka.min.css">
<script defer src="/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css%25!%28EXTRA%20string=solarized-light%29"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/dart.min.js"
     crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_2.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_2.png">

<meta name="description"
  content="The Gaussian discriminate analysis is a generative learning algorithm which models the PDF using a (multivariate) Gaussian distribution. This class of density estimation is also called parametric density estimation.
Parametric Density Estimation: To get an idea of the underlying distribution of the features we try to estimate this distribution based on a formula with parameters. Most commenly used formula for parametric density estimation is the Gaussian distribution:
1D Gaussian
 \[\begin{aligned} \mathcal{N}(\mu, \sigma^{2}) = \dfrac{1}{\sigma \sqrt{2\pi}} \exp{(-\dfrac{1}{2} ( \dfrac{x-\mu}{\sigma})^{2})} \end{aligned}\]  Multivariate Gaussian">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Gaussian Discriminative Analysis",
      "item":"/posts/2-gaussian-discriminative-analysis/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/2-gaussian-discriminative-analysis/"
    },
    "headline": "Gaussian Discriminative Analysis | ME","datePublished": "2019-12-20T20:24:51+01:00",
    "dateModified": "2019-12-20T20:24:51+01:00",
    "wordCount":  561 ,
    "publisher": {
        "@type": "Person",
        "name": "C. Wang",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "The Gaussian discriminate analysis is a generative learning algorithm which models the PDF using a (multivariate) Gaussian distribution. This class of density estimation is also called parametric density estimation.\nParametric Density Estimation: To get an idea of the underlying distribution of the features we try to estimate this distribution based on a formula with parameters. Most commenly used formula for parametric density estimation is the Gaussian distribution:\n1D Gaussian\n \\[\\begin{aligned} \\mathcal{N}(\\mu, \\sigma^{2}) = \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \\exp{(-\\dfrac{1}{2} ( \\dfrac{x-\\mu}{\\sigma})^{2})} \\end{aligned}\\]  Multivariate Gaussian"
}
</script><meta property="og:title" content="Gaussian Discriminative Analysis | ME" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/2-gaussian-discriminative-analysis/" />




<meta property="og:description" content="The Gaussian discriminate analysis is a generative learning algorithm which models the PDF using a (multivariate) Gaussian distribution. This class of density estimation is also called parametric density estimation.
Parametric Density Estimation: To get an idea of the underlying distribution of the features we try to estimate this distribution based on a formula with parameters. Most commenly used formula for parametric density estimation is the Gaussian distribution:
1D Gaussian
 \[\begin{aligned} \mathcal{N}(\mu, \sigma^{2}) = \dfrac{1}{\sigma \sqrt{2\pi}} \exp{(-\dfrac{1}{2} ( \dfrac{x-\mu}{\sigma})^{2})} \end{aligned}\]  Multivariate Gaussian" />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="ME" />






<meta property="article:published_time" content="2019-12-20T20:24:51&#43;01:00" />


<meta property="article:modified_time" content="2019-12-20T20:24:51&#43;01:00" />



<meta property="article:section" content="posts" />




<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="mr-6 text-primary-text text-xl font-bold">ME</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">About</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  mr-4">Posts</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka">Light</span>
                    <span class="px-4 py-1 hover:text-eureka">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            switchMode('Auto')
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }
    
    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script></div>
  </header>
  <main class="flex-grow pt-16">
    <div class="pl-scrollbar">
      <div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto">


<div class="grid grid-cols-2 lg:grid-cols-8 gap-4 lg:pt-12">
    <div
        class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded px-6 py-8">
        <h1 class="font-bold text-3xl text-primary-text">Gaussian Discriminative Analysis</h1>
        <div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text">
    <div class="mr-6 my-2">
        <i class="fas fa-calendar mr-1"></i>
        <span>2019-12-20</span>
    </div>
    <div class="mr-6 my-2">
        <i class="fas fa-clock mr-1"></i>
        <span>3 min read</span>
    </div>
    
    

    
</div>
        
        
        

        <div class="content">
            <p>The Gaussian discriminate analysis is a generative learning algorithm which models the PDF using a (multivariate) Gaussian distribution. This class of density estimation is also called parametric density estimation.</p>
<p><strong><em>Parametric Density Estimation</em></strong>: To get an idea of the underlying distribution of the features we try to estimate this distribution based on a formula with parameters. Most commenly used formula for parametric density estimation is the Gaussian distribution:</p>
<p><strong>1D Gaussian</strong></p>
<div>
\[\begin{aligned}
      \mathcal{N}(\mu, \sigma^{2}) = \dfrac{1}{\sigma \sqrt{2\pi}} \exp{(-\dfrac{1}{2} ( \dfrac{x-\mu}{\sigma})^{2})}
\end{aligned}\]
</div>
<p><img src="/images/Gauss1D.png" alt="img"></p>
<p><strong>Multivariate Gaussian</strong></p>
<div>
\[\begin{aligned}
      \mathcal{N}(\mu, \Sigma) = \dfrac{1}{(2\pi)^{n/2} |\Sigma^{1/2}| } \exp{(-\dfrac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x-\mu) )}
\end{aligned}\]
</div>
<p><img src="/images/Gauss2D.png" alt="img"></p>
<h3 id="quadratic-discriminate-analysis">Quadratic discriminate analysis</h3>
<p>Quadratic discriminate analysis is a form of GDA where the class conditional probability of each class is modeled using individual gaussians, which means that $\sigma$ of each class are not necessarily the same.</p>
<p>In binary (two class) QDA the classification is based on the discriminant:</p>
<div>
\[\begin{aligned}
    f(x) = \text{log} \ p(y_1 | x) - \text{ log } \ p(y_2 | x)
\end{aligned}\]
</div>
<p>A positive discriminant means the posterior probability of class 1 is higher and thus we classify $x$ as class 1. The log of the posterior probabilities is taken because this yields a simpler (more efficient) equation. This is possible because log is a monotonically increasing function, which means that the relations of order are unchanged. The $max_i$ $p(y_i|x)$ will also be the $max_i$ log $p(y_i|x)$. From the log of the posterior probability follows:</p>
<div>
\[\begin{aligned}
    p(y_i|x) &= \dfrac{p(x|y_i)p(y_i)}{p(x)} \\
    \log{p(y_i|x)} &= \log{p(x|y_i)} + \log{p(y_i)} - \log{p(x)} \\
    &= -\dfrac{p}{2}\log{2\pi} -\dfrac{1}{2}\log{\det{\Sigma_i}} - \dfrac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) \\
    &+ \log{p(y_i)} - \log{p(x)} \\
\end{aligned}\]
</div>
<p>Removing all non-class dependent constants yields:</p>
<div>
\[\begin{aligned}
    \log{p(y_i|x)} &= -\dfrac{1}{2}\log{\det{\Sigma_i}} - \dfrac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) + \log{p(y_i)}
\end{aligned}\]
</div>
<p>Plugging the result into the discriminant equation yields:</p>
<div>
\[\begin{aligned}
    f(x) &= \log{p(y_1|x)} - \log{p(y_2|x)} \\
    f(x) &= x^T W x + w^T x + w_0 \\
    W &= \dfrac{1}{2} (\Sigma_2^{-1} - \Sigma_1^{-1}) \\
    w &= \mu_1^T\Sigma_1^{-1} - \mu_2^T\Sigma_2^{-1} \\
    w_0 &= -\dfrac{1}{2}\log{\det{\Sigma_1}} - \dfrac{1}{2}\mu_1^T\Sigma_1^{-1}\mu_1 + \log{p(y_1)} \\
    &+ \dfrac{1}{2}\log{\det{\Sigma_2}} + \dfrac{1}{2}\mu_2^T\Sigma_2^{-1}\mu_2 - \log{p(y_2)} \\
\end{aligned}\]
</div>
<p>This is a quadratic equation modeling the decision boundary for a two class classification problem. The final class prediction or hypothesis $h(x)$ is then given by the following bernouilli distribution.</p>
<h3 id="linear-discriminate-analysis">Linear discriminate analysis</h3>
<p>Given the assumption that the covariance matrices of both classes are equal $(\Sigma_1 == \Sigma_2)$, the decision boundary becomes
linear:</p>
<div>
\[\begin{aligned}
    f(x) &= w^T x + w_0 \\
    w &= \hat{\Sigma}^{-1} (\hat{\mu}_2 - \hat{\mu}_1) \\
    w_0 &= \dfrac{1}{2} \hat{\mu}_2^T \hat{\Sigma}^{-1} \hat{\mu}_2 - \dfrac{1}{2} \hat{\mu}_1^T \hat{\Sigma}^{-1} \hat{\mu}_1 + \log{\dfrac{p(y_1)}{p(y_2)}} \\
\end{aligned}\]
</div>
<p>Also called Fisher&rsquo;s Linear Discriminate analysis.</p>
<h4 id="lda-example">LDA example</h4>
<p><img src="/images/MNIST_mean_std_plot.png" alt="img"></p>
<h4 id="lda-as-a-dimensionality-reduction-technique">LDA as a dimensionality reduction technique</h4>
<p>In Fisher&rsquo;s article he observes that at the decision boundary (discriminant = 0), the ratio of between and within class
variance is:</p>
<div>
\[\begin{aligned}
    \Phi = \dfrac{\text{variance between}}{\text{variance within}} = \dfrac{[\omega \mu_1 - \omega \mu_0]^2}{\omega^T \Sigma_1 \omega + \omega^T \Sigma_0 \omega}
\end{aligned}\]
</div>
<p>Here $\omega$ is a unit vector in a certain direction. He furthermore obeserves that this ratio is maximal for, $\omega$ as in LDA:</p>
<div>
\[\begin{aligned}
    \omega &= \hat{\Sigma}^{-1} (\hat{\mu}_2 - \hat{\mu}_1) \\
\end{aligned}\]
</div>
<p>The simple interpretation of this is that, the dicision boundary has a slope which is equal to the direction of maximum $\Phi$.</p>
<p>The use LDA as a dimensionality reduction technique all you have to do is map the data onto the first $k$ principal axes of the $\Phi$. (Later on we will have a look at other similar dimensionality reduction technique such as PCA)</p>

        </div>
        
        
        
        
        
        
        
<div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t">
    <div>
        
        <span class="block font-bold">Previous</span>
        <a href="/posts/3-non-parmetric-algorithms/" class="block">Non Parametric Algorithms</a>
        
    </div>
    <div class="md:text-right mt-4 md:mt-0">
        
        <span class="block font-bold">Next</span>
        <a href="/posts/1-machine-learning/" class="block">Machine Learning</a>
        
    </div>
</div>

        
    </div>
    

    
    
</div>
<script>
    document.addEventListener('DOMContentLoaded', ()=>{
        hljs.initHighlightingOnLoad();
    })
</script>

      </div>
    </div>
    
  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://www.wangchucheng.com/">C. Wang</a> and <a href="https://www.ruiqima.com/">R. Ma</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>
<!DOCTYPE html>
<html lang='en' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>Non Parametric Algorithms | ME</title>
<link rel="stylesheet" href="/css/eureka.min.css">
<script defer src="/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css%25!%28EXTRA%20string=solarized-light%29"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/dart.min.js"
     crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_2.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_2.png">

<meta name="description"
  content="As we have seen with the QDA and LDA classifiers, it is possible to obtain the posterior probabilities by estimating the class conditional probabilities using a (multivariate) Gaussian distribution. And we estimated the parameters of the Gaussian distribution, $\mu$ and $\Sigma$ based on set of (unbiased) examples.
Instead of using this parametric approach, it is also possible to estimate the class conditional probabilities using Kernel density estimation
Kernel density estimation: Here the main idea is that you approximate the distribution by a mixture of continuous distributions called kernels.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Non Parametric Algorithms",
      "item":"/posts/3-non-parmetric-algorithms/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/3-non-parmetric-algorithms/"
    },
    "headline": "Non Parametric Algorithms | ME","datePublished": "2019-12-22T20:24:51+01:00",
    "dateModified": "2019-12-22T20:24:51+01:00",
    "wordCount":  542 ,
    "publisher": {
        "@type": "Person",
        "name": "C. Wang",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "As we have seen with the QDA and LDA classifiers, it is possible to obtain the posterior probabilities by estimating the class conditional probabilities using a (multivariate) Gaussian distribution. And we estimated the parameters of the Gaussian distribution, $\\mu$ and $\\Sigma$ based on set of (unbiased) examples.\nInstead of using this parametric approach, it is also possible to estimate the class conditional probabilities using Kernel density estimation\nKernel density estimation: Here the main idea is that you approximate the distribution by a mixture of continuous distributions called kernels."
}
</script><meta property="og:title" content="Non Parametric Algorithms | ME" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/3-non-parmetric-algorithms/" />




<meta property="og:description" content="As we have seen with the QDA and LDA classifiers, it is possible to obtain the posterior probabilities by estimating the class conditional probabilities using a (multivariate) Gaussian distribution. And we estimated the parameters of the Gaussian distribution, $\mu$ and $\Sigma$ based on set of (unbiased) examples.
Instead of using this parametric approach, it is also possible to estimate the class conditional probabilities using Kernel density estimation
Kernel density estimation: Here the main idea is that you approximate the distribution by a mixture of continuous distributions called kernels." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="ME" />






<meta property="article:published_time" content="2019-12-22T20:24:51&#43;01:00" />


<meta property="article:modified_time" content="2019-12-22T20:24:51&#43;01:00" />



<meta property="article:section" content="posts" />




<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="mr-6 text-primary-text text-xl font-bold">ME</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">About</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  mr-4">Posts</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka">Light</span>
                    <span class="px-4 py-1 hover:text-eureka">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            switchMode('Auto')
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }
    
    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script></div>
  </header>
  <main class="flex-grow pt-16">
    <div class="pl-scrollbar">
      <div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto">


<div class="grid grid-cols-2 lg:grid-cols-8 gap-4 lg:pt-12">
    <div
        class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded px-6 py-8">
        <h1 class="font-bold text-3xl text-primary-text">Non Parametric Algorithms</h1>
        <div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text">
    <div class="mr-6 my-2">
        <i class="fas fa-calendar mr-1"></i>
        <span>2019-12-22</span>
    </div>
    <div class="mr-6 my-2">
        <i class="fas fa-clock mr-1"></i>
        <span>3 min read</span>
    </div>
    
    

    
</div>
        
        
        

        <div class="content">
            <p>As we have seen with the QDA and LDA classifiers, it is possible to obtain the posterior probabilities by estimating the class conditional probabilities using a (multivariate) Gaussian distribution. And we estimated the parameters of the Gaussian distribution, <strong>$\mu$</strong> and <strong>$\Sigma$</strong> based on set of (unbiased) examples.</p>
<p>Instead of using this parametric approach, it is also possible to estimate the class conditional probabilities using <strong>Kernel density estimation</strong></p>
<p><strong><em>Kernel density estimation:</em></strong> Here the main idea is that you approximate the distribution by a mixture of continuous distributions called kernels.</p>
<h3 id="histogram-density-estimation">Histogram Density Estimation</h3>
<p>In histogram density estimation the feature space is split in subregions (kernels) of width: $h$. Within each of these regions the number of objects of each class is counted. The class conditional probabilties are then calculated per region, and normalized by the kernel width and total amount of
samples.</p>
<div>
\[\begin{aligned}
    p(x|y_i) = \dfrac{1}{h} \dfrac{k_i}{n_i}
\end{aligned}\]
</div>
<p>The downside of the histogram method is that the feature space is discretely separated in bins. Therefore the estimated density will not be continuous throughout the feature space. Decreasing the bin size on the other hand causes problems in sparse datasets.</p>
<h3 id="parzen-density-estimation">Parzen Density Estimation</h3>
<p>In parzen density estimation, kernels are centered at every datatpoint $x_i$. The class conditional probability is the normalized sum of these kernels.</p>
<div>
\[\begin{aligned}
    p(x|y_i) = \dfrac{1}{n_i h} \sum_{i=1}^{n_i} K( \dfrac{x - x_j}{h})
\end{aligned}\]
</div>
<p>Given that the kernel used is wide enough and continuous, this method deals with the problems found in the histogram density
estimation.</p>
<h3 id="k-nn">K-NN</h3>
<p>Another method estimates the density of the distribution based on each data point $x$ his $k$ nearest neighbors. The metric for the density is based on the volume of a sphere $V_k$ centered at $x$ ,with radius $r$ being the distance to the $k^{th}$ nearest neighbor. This is divided by $n_i$ to normalize the density.</p>
<div>
\[\begin{aligned}
    p(x|y_i) &= \dfrac{k}{n_iV_{k_i}(x)} \\
    &= \dfrac{k}{n_i \pi (x-x_{k_i})^2 }
\end{aligned}\]
</div>
<p>The general hypothesis function for the two class case is as follows:</p>
<div>
\[\begin{aligned}
    h(x) =
    \begin{cases}
        \text{if} \ p(y_1 | x) > p(y_2 | x) &: y=1 \\
        \text{if} \ p(y_2 | x) > p(y_1 | x) &: y=2
    \end{cases}
\end{aligned}\]
</div>
<p>The data distribution is similar to the class distribution, only non-class dependent:</p>
<div>
\[\begin{aligned}
    p(x) &= \dfrac{k}{nV_k} \\
    &= \dfrac{k}{n \pi (x-x_k)^2}
\end{aligned}\]
</div>
<p>Class priors are given by:</p>
<div>
\[\begin{aligned}
    p(y_i) = \dfrac{n_i}{n}
\end{aligned}\]
</div>
<p>The class posteriors can be simplified, by removing non class dependant constants and multipliers.</p>
<div>
\[\begin{aligned}
    p(y_i|x) &= \dfrac{p(x|y_i)p(y_i)}{p(x)} \\
    &\Rightarrow p(x|y_i)p(y_i) \\
    &= \dfrac{k}{n \pi} \dfrac{1}{(x-x_{k_i})^2} \\
    &\Rightarrow \dfrac{1}{(x-x_{k_i})^2}
\end{aligned}\]
</div>
<p>This basically says that $x$ will be assigned to the class for which the euclidean distance to it&rsquo;s $k^{th}$ nearest neighbor is smallest. Plugging the result in the (bernouilli) hypothesis function yields:</p>
<div>
\[\begin{aligned}
    h(x) =&
    \begin{cases}
        \text{if} \ \dfrac{1}{(x-x_{k_1})^2} > \dfrac{1}{(x-x_{k_2})^2} &: y=1 \\
        \text{if} \ \dfrac{1}{(x-x_{k_2})^2} > \dfrac{1}{(x-x_{k_1})^2} &: y=2
    \end{cases} \\ \\
    =&
    \begin{cases}
        \text{if} \ (x-x_{k_2})^2 > (x-x_{k_1})^2 &: y=1 \\
        \text{if} \ (x-x_{k_1})^2 > (x-x_{k_2})^2 &: y=2 \\
    \end{cases}
\end{aligned}\]
</div>
<p>It is good to note that, unlike the gaussian function used in the parametric density estimations, the probability estimated using knn, parzen or histogram methods are improper. This means their area does not integrate to one for $-\infty$ to $+\infty$. Since we are only interested in the relations of order for our hypothesis function, this does not matter.
<a href="https://tex.stackexchange.com/questions/133932/how-do-i-type-the-infinity-symbol-in-mactex/133935">https://tex.stackexchange.com/questions/133932/how-do-i-type-the-infinity-symbol-in-mactex/133935</a></p>

        </div>
        
        
        
        
        
        
        
<div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t">
    <div>
        
        <span class="block font-bold">Previous</span>
        <a href="/posts/4-naive-bayes/" class="block">Naive Bayes</a>
        
    </div>
    <div class="md:text-right mt-4 md:mt-0">
        
        <span class="block font-bold">Next</span>
        <a href="/posts/2-gaussian-discriminative-analysis/" class="block">Gaussian Discriminative Analysis</a>
        
    </div>
</div>

        
    </div>
    

    
    
</div>
<script>
    document.addEventListener('DOMContentLoaded', ()=>{
        hljs.initHighlightingOnLoad();
    })
</script>

      </div>
    </div>
    
  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://www.wangchucheng.com/">C. Wang</a> and <a href="https://www.ruiqima.com/">R. Ma</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>
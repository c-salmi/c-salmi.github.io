<!DOCTYPE html>
<html lang='en' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>Linear Regression | ME</title>
<link rel="stylesheet" href="/css/eureka.min.css">
<script defer src="/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css%25!%28EXTRA%20string=solarized-light%29"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/languages/dart.min.js"
     crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
   integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3&#43;Aro6EYUG4&#43;cU&#43;KJWu/X"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" 
  integrity="sha384-g7c&#43;Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI&#43;sEnkvrMWph2EDg4"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
   integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC&#43;Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_2.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_2.png">

<meta name="description"
  content="The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Linear Regression",
      "item":"/posts/5-linear-regression/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/5-linear-regression/"
    },
    "headline": "Linear Regression | ME","datePublished": "2020-01-12T20:24:51+01:00",
    "dateModified": "2020-01-12T20:24:51+01:00",
    "wordCount":  739 ,
    "publisher": {
        "@type": "Person",
        "name": "C. Wang",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete."
}
</script><meta property="og:title" content="Linear Regression | ME" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/5-linear-regression/" />




<meta property="og:description" content="The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="ME" />






<meta property="article:published_time" content="2020-01-12T20:24:51&#43;01:00" />


<meta property="article:modified_time" content="2020-01-12T20:24:51&#43;01:00" />



<meta property="article:section" content="posts" />




<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="mr-6 text-primary-text text-xl font-bold">ME</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  mr-4">About</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  mr-4">Posts</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka">Light</span>
                    <span class="px-4 py-1 hover:text-eureka">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            switchMode('Auto')
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }
    
    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script></div>
  </header>
  <main class="flex-grow pt-16">
    <div class="pl-scrollbar">
      <div class="w-full max-w-screen-xl lg:px-4 xl:px-8 mx-auto">


<div class="grid grid-cols-2 lg:grid-cols-8 gap-4 lg:pt-12">
    <div
        class="col-span-2 lg:col-start-2 lg:col-span-6 bg-secondary-bg rounded px-6 py-8">
        <h1 class="font-bold text-3xl text-primary-text">Linear Regression</h1>
        <div class="flex flex-wrap flex-row items-center mt-2 text-tertiary-text">
    <div class="mr-6 my-2">
        <i class="fas fa-calendar mr-1"></i>
        <span>2020-01-12</span>
    </div>
    <div class="mr-6 my-2">
        <i class="fas fa-clock mr-1"></i>
        <span>4 min read</span>
    </div>
    
    

    
</div>
        
        
        

        <div class="content">
            <p>The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete. However for regression problems, where the label is continuous it has the potential to be much better. For a dataset containing 2 features (feature space of $\Re^2$)
this might look like:</p>
<div>
\[\begin{aligned}
     y = h_{\theta} (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
\end{aligned}\]
</div>
<blockquote>
<p>Note how the hypothesis function for the predicted output in literature is conventionally denoted by $h$.</p>
</blockquote>
<p>For an $n$ dimensional space this would equate to the following:</p>
<div>
\[\begin{aligned}
     y = h_{\theta}(x) = \sum_{i=0}^n \theta_i x_i = \theta^T x
\end{aligned}\]
</div>
<blockquote>
<p>Here $\theta_0$ is included in the statement by extending the feature vector $x$ for the bias term, so $x_0 = 1$.</p>
</blockquote>
<h4 id="cost-function">Cost function</h4>
<p>To learn a &lsquo;good&rsquo; weight vector $\theta$ we first need a certain metric for what is considered &lsquo;good&rsquo;. This is where the cost function comes in. There are different choices possible to use as cost function, but we will be talking about the <strong>least squares</strong> cost here. When we assume the probability of outcome $y$ given feature vector $x$ to be of a <strong>normal distribution</strong>, the max log likelihood can be taken to arrive at the following least squares cost function formulation:</p>
<div>
\[\begin{aligned}
     J(\theta) = \dfrac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2
\end{aligned}\]
</div>
<blockquote>
<p>Derivation can be found at <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></p>
</blockquote>
<h4 id="gradient-descend">Gradient descend</h4>
<p>From the cost function we can get a metric of how &lsquo;good&rsquo; our linear classifier is, because we want the distance to our hyperplane from both classes to be as high as possible. To obtain the best linear classifier to we need to optimize our weight vector $\theta$ with respect to our cost function.Since this is usually unfeasible to do analytically (because of high dimensionality and data volume) we will be using the optimization algorithm <strong>gradient descent</strong>. Here we move in the direction of lowest cost by repeatedly performing the update:</p>
<div>
\[\begin{aligned}
     \theta_j := \theta_j - \alpha \dfrac{\delta}{\delta \theta_j}  J(\theta)
\end{aligned}\]
</div>
<blockquote>
<p>Here $\alpha$ is also known as the learning rate. If $\alpha$ is too small gradient descent can be slow, if $\alpha$ is too large
gradient descend may fail to converge.</p>
</blockquote>
<h4 id="lms-algorithm">LMS algorithm</h4>
<p>The LMS algorithm uses the least squares cost function with the gradient descend optimizer. Let&rsquo;s first work it out for the
case of if we have only one training example (x,y), so that we can neglect the sum in the definition of J. We have:</p>
<div>
\[\begin{aligned}
     \dfrac{\delta}{\delta \theta_j} J(\theta) &= \dfrac{\delta}{\delta \theta_j} \dfrac{1}{2} (h_{\theta}(x) - y)^2 \\
     &= 2 \cdot \dfrac{1}{2} (h_{\theta}(x) - y) \cdot \dfrac{\delta}{\delta \theta_j} (h_{\theta}(x) - y) \\
     &= (h_{\theta}(x) - y) \cdot \dfrac{\delta}{\delta \theta_j} ( \sum_{i=0}^{n} \theta_i x_i - y) \\
     &= (h_{\theta}(x) - y) x_j
\end{aligned}\]
</div>
<p>For a single training example, this gives the update rule:</p>
<div>
\[\begin{aligned}
     \theta_j := \theta_j + \alpha (y-h_{\theta}(x))x_j
\end{aligned}\]
</div>
<p>This rule is called the <strong>LMS</strong> (least mean squared) update rule, and is also known as the <strong>Widrow-Hoff</strong> learning rule. This update rule has some
natural and intuitive properties. For instance the magnitude of the update is proportional to the error.
The update rule using all samples of the training set can be written as follows:</p>
<p>This is called <strong>batch gradient descend</strong>. You can imagine that calculating the error for every sample in the dataset for every
update step is very computationally expensive. Another version of gradient descend, called <strong>stochastic gradient descend</strong>
aims to deal with this problem, by stochastically sampling a single training example. It is good to note that this
update method will converge faster, however will never reach the optimum. Stochastic gradient descend is often used with large
datasets, because you can get very close to the optimum faster.</p>
<h4 id="usage">Usage</h4>
<p>Once $\theta$ has been &lsquo;trained&rsquo; properly, predictions can simple be made by the hypothesis function $h_{\theta}$.</p>
<blockquote>
<p>Calling the hypothesis function is practice is sometimes called running inference.</p>
</blockquote>
<p><em><strong>Final notes:</strong></em> Gradient descend optimisation techniques can be susceptible to local minima in the cost function. This is however not the case in our example, because the least squares cost function is a convex quadratic function (it has no local minima).</p>

        </div>
        
        
        
        
        
        
        
<div class="flex flex-col md:flex-row md:justify-between -mx-2 mt-4 px-2 pt-4 border-t">
    <div>
        
        <span class="block font-bold">Previous</span>
        <a href="/posts/6-logistic-regression/" class="block">Logistic Regression</a>
        
    </div>
    <div class="md:text-right mt-4 md:mt-0">
        
        <span class="block font-bold">Next</span>
        <a href="/posts/4-naive-bayes/" class="block">Naive Bayes</a>
        
    </div>
</div>

        
    </div>
    

    
    
</div>
<script>
    document.addEventListener('DOMContentLoaded', ()=>{
        hljs.initHighlightingOnLoad();
    })
</script>

      </div>
    </div>
    
  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://www.wangchucheng.com/">C. Wang</a> and <a href="https://www.ruiqima.com/">R. Ma</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>
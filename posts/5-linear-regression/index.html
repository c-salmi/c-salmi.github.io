<!DOCTYPE html>
<html
  lang="en"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>Linear Regression | ME</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css">
<script defer src="/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>













<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto&#43;Serif&#43;SC:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js"
     crossorigin></script>
<link rel="stylesheet" href="/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="/js/fontawesome.min.f14def4f6943de5b19b19c02e239b98c8cc6a8fe66b6828a21da7c0e9d405df8364cab68aa637c3bbe735eb7c325aa3c.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
   integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ"  media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" 
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY"  crossorigin></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
   integrity="sha384-&#43;XBljXPPiv&#43;OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"  crossorigin></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\(", right: "\\)", display: false },
        { left: "\\[", right: "\\]", display: true }
      ],
    });
  });
</script>


<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>


<link rel="icon" type="image/png" sizes="32x32" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/icon_hu64421c6c7700f606f0ad45d807017b09_5843_180x180_fill_box_center_3.png">

<meta name="description"
  content="The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete.">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Posts",
      "item":"/posts/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Linear Regression",
      "item":"/posts/5-linear-regression/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/posts/5-linear-regression/"
    },
    "headline": "Linear Regression | ME","datePublished": "2020-01-12T20:24:51+01:00",
    "dateModified": "2020-01-12T20:24:51+01:00",
    "wordCount":  739 ,
    "publisher": {
        "@type": "Person",
        "name": "WANG Chucheng",
        "logo": {
            "@type": "ImageObject",
            "url": "/images/icon.png"
        }
        },
    "description": "The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete."
}
</script><meta property="og:title" content="Linear Regression | ME" />
<meta property="og:type" content="article" />


<meta property="og:image" content="/images/icon.png">


<meta property="og:url" content="/posts/5-linear-regression/" />




<meta property="og:description" content="The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete." />




<meta property="og:locale" content="en" />




<meta property="og:site_name" content="ME" />






<meta property="article:published_time" content="2020-01-12T20:24:51&#43;01:00" />


<meta property="article:modified_time" content="2020-01-12T20:24:51&#43;01:00" />



<meta property="article:section" content="posts" />





  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">ME</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/#about" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">About</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">Posts</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">Light</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8">
  
  
  <div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12">
    <div
      class="lg:col-start-2 bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"
    >
      <article class="prose">
  <h1 class="mb-4">Linear Regression</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2020-01-12</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>4 min read</span>
  </div>

  

  
</div>


  
  

  <p>The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete. However for regression problems, where the label is continuous it has the potential to be much better. For a dataset containing 2 features (feature space of $\Re^2$)
this might look like:</p>
<div>
\[\begin{aligned}
     y = h_{\theta} (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2
\end{aligned}\]
</div>
<blockquote>
<p>Note how the hypothesis function for the predicted output in literature is conventionally denoted by $h$.</p>
</blockquote>
<p>For an $n$ dimensional space this would equate to the following:</p>
<div>
\[\begin{aligned}
     y = h_{\theta}(x) = \sum_{i=0}^n \theta_i x_i = \theta^T x
\end{aligned}\]
</div>
<blockquote>
<p>Here $\theta_0$ is included in the statement by extending the feature vector $x$ for the bias term, so $x_0 = 1$.</p>
</blockquote>
<h4 id="cost-function">Cost function</h4>
<p>To learn a &lsquo;good&rsquo; weight vector $\theta$ we first need a certain metric for what is considered &lsquo;good&rsquo;. This is where the cost function comes in. There are different choices possible to use as cost function, but we will be talking about the <strong>least squares</strong> cost here. When we assume the probability of outcome $y$ given feature vector $x$ to be of a <strong>normal distribution</strong>, the max log likelihood can be taken to arrive at the following least squares cost function formulation:</p>
<div>
\[\begin{aligned}
     J(\theta) = \dfrac{1}{2} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2
\end{aligned}\]
</div>
<blockquote>
<p>Derivation can be found at <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></p>
</blockquote>
<h4 id="gradient-descend">Gradient descend</h4>
<p>From the cost function we can get a metric of how &lsquo;good&rsquo; our linear classifier is, because we want the distance to our hyperplane from both classes to be as high as possible. To obtain the best linear classifier to we need to optimize our weight vector $\theta$ with respect to our cost function.Since this is usually unfeasible to do analytically (because of high dimensionality and data volume) we will be using the optimization algorithm <strong>gradient descent</strong>. Here we move in the direction of lowest cost by repeatedly performing the update:</p>
<div>
\[\begin{aligned}
     \theta_j := \theta_j - \alpha \dfrac{\delta}{\delta \theta_j}  J(\theta)
\end{aligned}\]
</div>
<blockquote>
<p>Here $\alpha$ is also known as the learning rate. If $\alpha$ is too small gradient descent can be slow, if $\alpha$ is too large
gradient descend may fail to converge.</p>
</blockquote>
<h4 id="lms-algorithm">LMS algorithm</h4>
<p>The LMS algorithm uses the least squares cost function with the gradient descend optimizer. Let&rsquo;s first work it out for the
case of if we have only one training example (x,y), so that we can neglect the sum in the definition of J. We have:</p>
<div>
\[\begin{aligned}
     \dfrac{\delta}{\delta \theta_j} J(\theta) &= \dfrac{\delta}{\delta \theta_j} \dfrac{1}{2} (h_{\theta}(x) - y)^2 \\
     &= 2 \cdot \dfrac{1}{2} (h_{\theta}(x) - y) \cdot \dfrac{\delta}{\delta \theta_j} (h_{\theta}(x) - y) \\
     &= (h_{\theta}(x) - y) \cdot \dfrac{\delta}{\delta \theta_j} ( \sum_{i=0}^{n} \theta_i x_i - y) \\
     &= (h_{\theta}(x) - y) x_j
\end{aligned}\]
</div>
<p>For a single training example, this gives the update rule:</p>
<div>
\[\begin{aligned}
     \theta_j := \theta_j + \alpha (y-h_{\theta}(x))x_j
\end{aligned}\]
</div>
<p>This rule is called the <strong>LMS</strong> (least mean squared) update rule, and is also known as the <strong>Widrow-Hoff</strong> learning rule. This update rule has some
natural and intuitive properties. For instance the magnitude of the update is proportional to the error.
The update rule using all samples of the training set can be written as follows:</p>
<p>This is called <strong>batch gradient descend</strong>. You can imagine that calculating the error for every sample in the dataset for every
update step is very computationally expensive. Another version of gradient descend, called <strong>stochastic gradient descend</strong>
aims to deal with this problem, by stochastically sampling a single training example. It is good to note that this
update method will converge faster, however will never reach the optimum. Stochastic gradient descend is often used with large
datasets, because you can get very close to the optimum faster.</p>
<h4 id="usage">Usage</h4>
<p>Once $\theta$ has been &rsquo;trained&rsquo; properly, predictions can simple be made by the hypothesis function $h_{\theta}$.</p>
<blockquote>
<p>Calling the hypothesis function is practice is sometimes called running inference.</p>
</blockquote>
<p><em><strong>Final notes:</strong></em> Gradient descend optimisation techniques can be susceptible to local minima in the cost function. This is however not the case in our example, because the least squares cost function is a convex quadratic function (it has no local minima).</p>

</article>


      

      



      

      
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
        <span class="text-primary-text block font-bold"
          >Previous</span
        >
        <a href="/posts/6-logistic-regression/" class="block">Logistic Regression</a>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
        <span class="text-primary-text block font-bold">Next</span>
        <a href="/posts/4-naive-bayes/" class="block">Naive Bayes</a>
      
    </div>
  </div>


      



    </div>
    

    
    
  </div>

  
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        hljs.highlightAll();
      });
    </script>

          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://www.wangchucheng.com/">WANG Chucheng</a> and <a href="https://www.ruiqima.com/">MA Ruiqi</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
    </footer>
  </body>
</html>

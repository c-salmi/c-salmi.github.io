<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ME</title>
    <link>/</link>
    <description>Recent content on ME</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; 2021 &lt;a href=&#34;https://www.wangchucheng.com/&#34;&gt;C. Wang&lt;/a&gt; and &lt;a href=&#34;https://www.ruiqima.com/&#34;&gt;R. Ma&lt;/a&gt;
</copyright>
    <lastBuildDate>Fri, 01 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Formula student driverless</title>
      <link>/posts/9-robot-experiment/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/posts/9-robot-experiment/</guid>
      <description>What is formula student? Studying at the Mechanical Engineering building at the TU Delft, formula student is a well known term. In Delft, every year a group of highly motivated students put their studies on hold to build a race car completely from scratch! They don&amp;rsquo;t just build it for the heck of it, but the goal is to make a better car than competing univserities throughout the world and win first place in multiple student engineering competitions.</description>
    </item>
    
    <item>
      <title>Generalized Linear Models</title>
      <link>/posts/7-generalized-linear-models/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/7-generalized-linear-models/</guid>
      <description>In my previous posts I have talked about linear and logistic regression. Today I will talk about the broader family of models to which both methods belong Generalized Linear Models. To work our way up to GLMs, we will begin by defining the exponential family.
The exponential family: A class of distributions is in the exponential family if it can be written in the form:
$$ p(y;\eta) = b(y) \text{exp}(\eta^T T(y) - \alpha(\eta)) $$
 More information can be found here</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>/posts/6-logistic-regression/</link>
      <pubDate>Wed, 15 Jan 2020 20:24:51 +0100</pubDate>
      
      <guid>/posts/6-logistic-regression/</guid>
      <description>Logistic regression Logistic regression unlike linear regression is well suited for classification type problems. The difference lies in the way data is fitted, instead of a linear (hyper)plane it is fitted using the following exponential function (also known as the sigmoid function):
 \[\begin{aligned} h_{\theta}(x) = \dfrac{1}{1 + \exp{(-\theta^Tx)}} \end{aligned}\]  This function has the nice property that, its output is bound between zero and one. This is a usefull property when trying to model the (posterior) probability of something.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>/posts/5-linear-regression/</link>
      <pubDate>Sun, 12 Jan 2020 20:24:51 +0100</pubDate>
      
      <guid>/posts/5-linear-regression/</guid>
      <description>The linear regression algorithm works by trying to predict the class label directly given an object. It tries to fit a (hyper)plane as good as possible through all the training examples, so that the output of the hyperplane $y$ predicts the class label. As you may have noticed the label $y$ will be predicted as a continuous value on the hyper plane. This makes it a pretty horrible fit for a (binary) classification problem, where the class label is discrete.</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>/posts/4-naive-bayes/</link>
      <pubDate>Wed, 25 Dec 2019 20:24:51 +0100</pubDate>
      
      <guid>/posts/4-naive-bayes/</guid>
      <description>Naive Bayes All generative algorithms mentioned above try to model the multivariate class conditional distribution over all features. However when the shear amount of features becomes to large, the number of parameters needed to model the distribution will become infeasible in practice. A Gaussian function for example scales $O(n + n^2) \simeq O(n^2)$ with amount of features.
The Naive Bayes classifier will therefore make a very strong assumption. All $x_i&amp;rsquo;s$ are considered conditionally independent given $y$.</description>
    </item>
    
    <item>
      <title>Non Parametric Algorithms</title>
      <link>/posts/3-non-parmetric-algorithms/</link>
      <pubDate>Sun, 22 Dec 2019 20:24:51 +0100</pubDate>
      
      <guid>/posts/3-non-parmetric-algorithms/</guid>
      <description>As we have seen with the QDA and LDA classifiers, it is possible to obtain the posterior probabilities by estimating the class conditional probabilities using a (multivariate) Gaussian distribution. And we estimated the parameters of the Gaussian distribution, $\mu$ and $\Sigma$ based on set of (unbiased) examples.
Instead of using this parametric approach, it is also possible to estimate the class conditional probabilities using Kernel density estimation
Kernel density estimation: Here the main idea is that you approximate the distribution by a mixture of continuous distributions called kernels.</description>
    </item>
    
    <item>
      <title>Gaussian Discriminative Analysis</title>
      <link>/posts/2-gaussian-discriminative-analysis/</link>
      <pubDate>Fri, 20 Dec 2019 20:24:51 +0100</pubDate>
      
      <guid>/posts/2-gaussian-discriminative-analysis/</guid>
      <description>If I sample many many apples from a basket of independent identically distributed (i.i.d.) randomly sized appels, the distribution of the size of my apples will converge towards the Gaussian distribution. This is what Laplace came up with in 1812, in his “Analytic Theory of Probability”. Although Carl Friedrich Gauss 3 years before that already introduced the function and worked out the least squares approximation to estimate the &amp;lsquo;true&amp;rsquo; (mean) value from a bunch of i.</description>
    </item>
    
    <item>
      <title>Machine Learning</title>
      <link>/posts/1-machine-learning/</link>
      <pubDate>Sun, 24 Nov 2019 20:24:51 +0100</pubDate>
      
      <guid>/posts/1-machine-learning/</guid>
      <description>Hello world, this is the first in a series of posts about machine learning. I&amp;rsquo;m mostly making these posts for fun and to go through the basics again, wipe away the dust. I aspire to always think from first principles, so knowing the first principles might come in handy some time.
So lets talk about machine learning &amp;hellip; Machin learning basically consists of the detection of pattern in data. Data usually refers to a collection of features and classes.</description>
    </item>
    
  </channel>
</rss>
